FANN_FLO_2.1
num_layers=7
learning_rate=0.350000
connection_rate=1.000000
network_type=1
learning_momentum=0.000000
training_algorithm=2
train_error_function=0
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=8
bit_fail_limit=8.99999976158142090000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000005960464480000e-001
cascade_activation_functions_count=1
cascade_activation_functions=5 
cascade_activation_steepnesses_count=1
cascade_activation_steepnesses=1.00000000000000000000e+000 
layer_sizes=9 1 1 1 1 1 1 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 5, 1.00000000000000000000e+000) (10, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (12, 5, 1.00000000000000000000e+000) (13, 5, 1.00000000000000000000e+000) (14, 0, 5.00000000000000000000e-001) 
connections (connected_to_neuron, weight)=(0, 1.78335797786712650000e+000) (1, 1.78417003154754640000e+000) (2, 1.78352427482604980000e+000) (3, 1.78353679180145260000e+000) (4, 1.78345119953155520000e+000) (5, 1.78343105316162110000e+000) (6, 1.78286290168762210000e+000) (7, 1.78344988822937010000e+000) (8, -1.78338932991027830000e+000) (0, -1.55138611793518070000e+000) (1, -1.54876470565795900000e+000) (2, -1.54740488529205320000e+000) (3, -1.54309868812561040000e+000) (4, -1.54727959632873540000e+000) (5, -1.54895472526550290000e+000) (6, -1.55145835876464840000e+000) (7, -1.54713737964630130000e+000) (8, 1.55160105228424070000e+000) (9, 2.79206705093383790000e+000) (0, 1.25549447536468510000e+000) (1, 1.24300062656402590000e+000) (2, 1.24849009513854980000e+000) (3, 1.24651360511779790000e+000) (4, 1.24752974510192870000e+000) (5, 1.24127697944641110000e+000) (6, 1.25581991672515870000e+000) (7, 1.25082504749298100000e+000) (8, -1.25225186347961430000e+000) (9, -2.70107436180114750000e+000) (10, 1.66653895378112790000e+000) (0, -3.48077535629272460000e-001) (1, -3.45089077949523930000e-001) (2, -3.46417754888534550000e-001) (3, -3.46397787332534790000e-001) (4, -3.41018617153167720000e-001) (5, -3.44003617763519290000e-001) (6, -3.50642681121826170000e-001) (7, -3.48709046840667720000e-001) (8, 3.46459865570068360000e-001) (9, 7.99535751342773440000e-001) (10, -5.29590249061584470000e-001) (11, 5.44430255889892580000e-001) (0, 4.33022499084472660000e-001) (1, 4.29619848728179930000e-001) (2, 4.30875569581985470000e-001) (3, 4.31099325418472290000e-001) (4, 4.23284530639648440000e-001) (5, 4.28402900695800780000e-001) (6, 4.36465710401535030000e-001) (7, 4.33956831693649290000e-001) (8, -4.30792242288589480000e-001) (9, -9.97002422809600830000e-001) (10, 6.61991417407989500000e-001) (11, -6.89811944961547850000e-001) (12, 1.43711328506469730000e+000) (0, -7.31554508209228520000e-001) (1, -4.82985317707061770000e-001) (2, -7.08055496215820310000e-001) (3, -7.14173734188079830000e-001) (4, -7.18264698982238770000e-001) (5, -6.91845953464508060000e-001) (6, -4.95041251182556150000e-001) (7, -4.87290024757385250000e-001) (8, 7.31213748455047610000e-001) (9, 2.02112197875976560000e+000) (10, 6.00054681301116940000e-001) (11, 3.40787601470947270000e+000) (12, 3.07409334182739260000e+000) (13, 1.42076969146728520000e+001) 
