FANN_FLO_2.1
num_layers=8
learning_rate=0.350000
connection_rate=1.000000
network_type=1
learning_momentum=0.000000
training_algorithm=2
train_error_function=0
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=8
bit_fail_limit=8.99999976158142090000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000005960464480000e-001
cascade_activation_functions_count=1
cascade_activation_functions=5 
cascade_activation_steepnesses_count=1
cascade_activation_steepnesses=1.00000000000000000000e+000 
layer_sizes=9 1 1 1 1 1 1 1 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 5, 1.00000000000000000000e+000) (10, 5, 1.00000000000000000000e+000) (11, 5, 1.00000000000000000000e+000) (12, 5, 1.00000000000000000000e+000) (13, 5, 1.00000000000000000000e+000) (14, 5, 1.00000000000000000000e+000) (15, 0, 5.00000000000000000000e-001) 
connections (connected_to_neuron, weight)=(0, 1.78307938575744630000e+000) (1, 1.78322076797485350000e+000) (2, 1.78355050086975100000e+000) (3, 1.78483819961547850000e+000) (4, 1.78428268432617190000e+000) (5, 1.78319501876831050000e+000) (6, 1.78266179561614990000e+000) (7, 1.78429567813873290000e+000) (8, -1.78228557109832760000e+000) (0, -1.56335067749023440000e+000) (1, -1.56354880332946780000e+000) (2, -1.55827784538269040000e+000) (3, -1.55864787101745610000e+000) (4, -1.55811429023742680000e+000) (5, -1.56264388561248780000e+000) (6, -1.55805861949920650000e+000) (7, -1.56274926662445070000e+000) (8, 1.56346833705902100000e+000) (9, 2.81872653961181640000e+000) (0, 2.23024153709411620000e+000) (1, 2.22898244857788090000e+000) (2, 2.21879100799560550000e+000) (3, 2.21975421905517580000e+000) (4, 2.22427058219909670000e+000) (5, 2.23424100875854490000e+000) (6, 2.21920251846313480000e+000) (7, 2.23389196395874020000e+000) (8, -2.22395038604736330000e+000) (9, -4.68632173538208010000e+000) (10, 2.75700592994689940000e+000) (0, 6.15916192531585690000e-001) (1, 6.14470720291137700000e-001) (2, 6.08897805213928220000e-001) (3, 6.08620047569274900000e-001) (4, 6.12781047821044920000e-001) (5, 6.18004381656646730000e-001) (6, 6.08323514461517330000e-001) (7, 6.16965532302856450000e-001) (8, -6.02236688137054440000e-001) (9, -1.44209051132202150000e+000) (10, 9.45837497711181640000e-001) (11, -8.87930631637573240000e-001) (0, 6.69698566198349000000e-002) (1, 1.17066383361816410000e-001) (2, 1.11197344958782200000e-001) (3, 7.70021453499794010000e-002) (4, 1.15820810198783870000e-001) (5, 1.23155146837234500000e-001) (6, 8.53172689676284790000e-002) (7, 1.20167441666126250000e-001) (8, -5.61461746692657470000e-002) (9, -2.06054359674453740000e-001) (10, 9.49476286768913270000e-002) (11, -1.67493075132370000000e-002) (12, 2.33913969993591310000e+000) (0, 7.16741621494293210000e-001) (1, 1.06949830055236820000e+000) (2, 1.04124140739440920000e+000) (3, 7.77939379215240480000e-001) (4, 1.04855346679687500000e+000) (5, 1.09481310844421390000e+000) (6, 8.22276890277862550000e-001) (7, 1.06243467330932620000e+000) (8, -5.61850428581237790000e-001) (9, -2.33670830726623540000e+000) (10, 1.46517217159271240000e+000) (11, -1.64268136024475100000e+000) (12, 5.52274560928344730000e+000) (13, -6.71770191192626950000e+000) (0, -3.35004270076751710000e-001) (1, -1.85617238283157350000e-001) (2, -1.61121666431427000000e-001) (3, -2.83918738365173340000e-001) (4, -3.47993910312652590000e-001) (5, -9.46032106876373290000e-002) (6, -2.46193856000900270000e-001) (7, -1.12258762121200560000e-001) (8, 6.47155165672302250000e-001) (9, 1.53113579750061040000e+000) (10, 9.64447259902954100000e-001) (11, 2.56414556503295900000e+000) (12, 4.06095647811889650000e+000) (13, -4.48233652114868160000e+000) (14, 1.34579837322235110000e+000) 
